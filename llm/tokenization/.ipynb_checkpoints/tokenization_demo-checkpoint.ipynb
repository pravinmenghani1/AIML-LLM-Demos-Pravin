{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization in Large Language Models: A Complete Guide\n",
    "\n",
    "## Problem Statement\n",
    "Understanding how text is converted into numerical representations that LLMs can process is crucial for:\n",
    "- Optimizing prompt engineering\n",
    "- Managing token limits and costs\n",
    "- Understanding model behavior and limitations\n",
    "- Building efficient text processing pipelines\n",
    "\n",
    "This notebook demonstrates different tokenization methods, their trade-offs, and practical implications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Tokenization?\n",
    "\n",
    "Tokenization is the process of breaking down text into smaller units (tokens) that can be processed by machine learning models. In LLMs, tokens are the basic units of input and output.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Token**: A unit of text (word, subword, or character)\n",
    "- **Vocabulary**: Set of all possible tokens the model knows\n",
    "- **Token ID**: Numerical representation of each token\n",
    "- **Out-of-Vocabulary (OOV)**: Tokens not in the model's vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers tiktoken sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tiktoken\n",
    "from transformers import AutoTokenizer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Tokenization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text for demonstration\n",
    "sample_text = \"Hello world! This is a demonstration of tokenization in LLMs. It's fascinating how models understand text.\"\n",
    "\n",
    "print(f\"Original text: {sample_text}\")\n",
    "print(f\"Text length: {len(sample_text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Word-Level Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(text):\n",
    "    # Simple word tokenization using regex\n",
    "    tokens = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text.lower())\n",
    "    return tokens\n",
    "\n",
    "word_tokens = word_tokenize(sample_text)\n",
    "print(f\"Word tokens: {word_tokens}\")\n",
    "print(f\"Number of tokens: {len(word_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Character-Level Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_tokenize(text):\n",
    "    return list(text)\n",
    "\n",
    "char_tokens = char_tokenize(sample_text)\n",
    "print(f\"Character tokens: {char_tokens[:20]}...\")  # Show first 20\n",
    "print(f\"Number of tokens: {len(char_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modern Tokenization: Byte Pair Encoding (BPE)\n",
    "\n",
    "BPE is a subword tokenization method that balances vocabulary size with meaningful representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Simple BPE Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBPE:\n",
    "    def __init__(self, vocab_size=1000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {}\n",
    "        self.merges = {}\n",
    "    \n",
    "    def get_pairs(self, word):\n",
    "        pairs = set()\n",
    "        prev_char = word[0]\n",
    "        for char in word[1:]:\n",
    "            pairs.add((prev_char, char))\n",
    "            prev_char = char\n",
    "        return pairs\n",
    "    \n",
    "    def train(self, corpus):\n",
    "        # Initialize vocabulary with characters\n",
    "        vocab = set()\n",
    "        word_freqs = Counter(corpus.split())\n",
    "        \n",
    "        # Convert words to character sequences\n",
    "        word_splits = {}\n",
    "        for word in word_freqs:\n",
    "            word_splits[word] = list(word)\n",
    "            vocab.update(word_splits[word])\n",
    "        \n",
    "        # Learn merges\n",
    "        num_merges = self.vocab_size - len(vocab)\n",
    "        \n",
    "        for i in range(num_merges):\n",
    "            pairs = {}\n",
    "            for word, freq in word_freqs.items():\n",
    "                word_pairs = self.get_pairs(word_splits[word])\n",
    "                for pair in word_pairs:\n",
    "                    pairs[pair] = pairs.get(pair, 0) + freq\n",
    "            \n",
    "            if not pairs:\n",
    "                break\n",
    "                \n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            self.merges[best_pair] = len(vocab)\n",
    "            vocab.add(''.join(best_pair))\n",
    "            \n",
    "            # Update word splits\n",
    "            for word in word_splits:\n",
    "                new_word = []\n",
    "                i = 0\n",
    "                while i < len(word_splits[word]):\n",
    "                    if (i < len(word_splits[word]) - 1 and \n",
    "                        (word_splits[word][i], word_splits[word][i+1]) == best_pair):\n",
    "                        new_word.append(''.join(best_pair))\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_word.append(word_splits[word][i])\n",
    "                        i += 1\n",
    "                word_splits[word] = new_word\n",
    "        \n",
    "        self.vocab = {token: i for i, token in enumerate(vocab)}\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        tokens = []\n",
    "        for word in text.split():\n",
    "            word_tokens = list(word)\n",
    "            \n",
    "            # Apply learned merges\n",
    "            for pair in self.merges:\n",
    "                new_tokens = []\n",
    "                i = 0\n",
    "                while i < len(word_tokens):\n",
    "                    if (i < len(word_tokens) - 1 and \n",
    "                        (word_tokens[i], word_tokens[i+1]) == pair):\n",
    "                        new_tokens.append(''.join(pair))\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_tokens.append(word_tokens[i])\n",
    "                        i += 1\n",
    "                word_tokens = new_tokens\n",
    "            \n",
    "            tokens.extend(word_tokens)\n",
    "        return tokens\n",
    "\n",
    "# Train simple BPE\n",
    "corpus = \"hello world this is a simple example of tokenization\"\n",
    "bpe = SimpleBPE(vocab_size=50)\n",
    "bpe.train(corpus)\n",
    "\n",
    "bpe_tokens = bpe.tokenize(\"hello world\")\n",
    "print(f\"BPE tokens: {bpe_tokens}\")\n",
    "print(f\"Learned merges: {list(bpe.merges.keys())[:5]}...\")  # Show first 5 merges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Real-World Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 OpenAI's tiktoken (GPT models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4 tokenizer\n",
    "gpt4_tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "gpt4_tokens = gpt4_tokenizer.encode(sample_text)\n",
    "gpt4_decoded = [gpt4_tokenizer.decode([token]) for token in gpt4_tokens]\n",
    "\n",
    "print(f\"GPT-4 tokens: {gpt4_tokens}\")\n",
    "print(f\"GPT-4 decoded: {gpt4_decoded}\")\n",
    "print(f\"Number of tokens: {len(gpt4_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Hugging Face Transformers (BERT, GPT-2, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT tokenizer\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_tokens = bert_tokenizer.tokenize(sample_text)\n",
    "bert_ids = bert_tokenizer.encode(sample_text)\n",
    "\n",
    "print(f\"BERT tokens: {bert_tokens}\")\n",
    "print(f\"BERT token IDs: {bert_ids}\")\n",
    "print(f\"Number of tokens: {len(bert_tokens)}\")\n",
    "\n",
    "# GPT-2 tokenizer\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "gpt2_tokens = gpt2_tokenizer.tokenize(sample_text)\n",
    "\n",
    "print(f\"\\nGPT-2 tokens: {gpt2_tokens}\")\n",
    "print(f\"Number of tokens: {len(gpt2_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tokenization Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different tokenization methods\n",
    "test_texts = [\n",
    "    \"Hello world!\",\n",
    "    \"Tokenization is fascinating.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"AI and machine learning are revolutionizing technology.\",\n",
    "    \"Supercalifragilisticexpialidocious\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "for text in test_texts:\n",
    "    word_count = len(word_tokenize(text))\n",
    "    char_count = len(char_tokenize(text))\n",
    "    gpt4_count = len(gpt4_tokenizer.encode(text))\n",
    "    bert_count = len(bert_tokenizer.tokenize(text))\n",
    "    gpt2_count = len(gpt2_tokenizer.tokenize(text))\n",
    "    \n",
    "    results.append({\n",
    "        'text': text,\n",
    "        'word': word_count,\n",
    "        'char': char_count,\n",
    "        'gpt4': gpt4_count,\n",
    "        'bert': bert_count,\n",
    "        'gpt2': gpt2_count\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "print(f\"{'Text':<50} {'Word':<6} {'Char':<6} {'GPT-4':<6} {'BERT':<6} {'GPT-2':<6}\")\n",
    "print(\"-\" * 80)\n",
    "for result in results:\n",
    "    print(f\"{result['text']:<50} {result['word']:<6} {result['char']:<6} {result['gpt4']:<6} {result['bert']:<6} {result['gpt2']:<6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization of Tokenization Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "methods = ['Word', 'Character', 'GPT-4', 'BERT', 'GPT-2']\n",
    "text_labels = [f\"Text {i+1}\" for i in range(len(test_texts))]\n",
    "\n",
    "word_counts = [r['word'] for r in results]\n",
    "char_counts = [r['char'] for r in results]\n",
    "gpt4_counts = [r['gpt4'] for r in results]\n",
    "bert_counts = [r['bert'] for r in results]\n",
    "gpt2_counts = [r['gpt2'] for r in results]\n",
    "\n",
    "x = np.arange(len(text_labels))\n",
    "width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.bar(x - 2*width, word_counts, width, label='Word', alpha=0.8)\n",
    "ax.bar(x - width, char_counts, width, label='Character', alpha=0.8)\n",
    "ax.bar(x, gpt4_counts, width, label='GPT-4', alpha=0.8)\n",
    "ax.bar(x + width, bert_counts, width, label='BERT', alpha=0.8)\n",
    "ax.bar(x + 2*width, gpt2_counts, width, label='GPT-2', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Test Texts')\n",
    "ax.set_ylabel('Number of Tokens')\n",
    "ax.set_title('Tokenization Comparison Across Different Methods')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(text_labels)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Special Tokens and Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate special tokens\n",
    "special_text = \"Hello! How are you? I'm fine. 😊 Check out https://example.com\"\n",
    "\n",
    "print(\"Special tokens and edge cases:\")\n",
    "print(f\"Text: {special_text}\")\n",
    "print()\n",
    "\n",
    "# GPT-4 handling\n",
    "gpt4_special = gpt4_tokenizer.encode(special_text)\n",
    "gpt4_decoded_special = [gpt4_tokenizer.decode([token]) for token in gpt4_special]\n",
    "print(f\"GPT-4 tokens: {gpt4_decoded_special}\")\n",
    "\n",
    "# BERT handling\n",
    "bert_special = bert_tokenizer.tokenize(special_text)\n",
    "print(f\"BERT tokens: {bert_special}\")\n",
    "\n",
    "# Show special tokens in BERT\n",
    "bert_with_special = bert_tokenizer.encode(special_text, add_special_tokens=True)\n",
    "bert_decoded_with_special = bert_tokenizer.convert_ids_to_tokens(bert_with_special)\n",
    "print(f\"BERT with special tokens: {bert_decoded_with_special}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practical Implications for LLM Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token counting for cost estimation\n",
    "def estimate_cost(text, model=\"gpt-4\", input_cost_per_1k=0.03, output_cost_per_1k=0.06):\n",
    "    \"\"\"Estimate API cost based on token count\"\"\"\n",
    "    if model == \"gpt-4\":\n",
    "        tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    else:\n",
    "        tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    \n",
    "    token_count = len(tokenizer.encode(text))\n",
    "    input_cost = (token_count / 1000) * input_cost_per_1k\n",
    "    \n",
    "    return {\n",
    "        'tokens': token_count,\n",
    "        'input_cost': input_cost,\n",
    "        'estimated_output_tokens': token_count * 0.5,  # Rough estimate\n",
    "        'estimated_total_cost': input_cost + ((token_count * 0.5) / 1000) * output_cost_per_1k\n",
    "    }\n",
    "\n",
    "# Example prompt\n",
    "long_prompt = \"\"\"\n",
    "You are an AI assistant helping with data analysis. \n",
    "Please analyze the following dataset and provide insights:\n",
    "\n",
    "Dataset: Sales data for Q1 2024\n",
    "- January: $50,000 revenue, 200 customers\n",
    "- February: $65,000 revenue, 250 customers  \n",
    "- March: $80,000 revenue, 320 customers\n",
    "\n",
    "Please provide:\n",
    "1. Growth rate analysis\n",
    "2. Customer acquisition trends\n",
    "3. Revenue per customer metrics\n",
    "4. Recommendations for Q2\n",
    "\"\"\"\n",
    "\n",
    "cost_analysis = estimate_cost(long_prompt)\n",
    "print(\"Cost Analysis for Sample Prompt:\")\n",
    "print(f\"Token count: {cost_analysis['tokens']}\")\n",
    "print(f\"Input cost: ${cost_analysis['input_cost']:.4f}\")\n",
    "print(f\"Estimated total cost: ${cost_analysis['estimated_total_cost']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices and Optimization Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate token optimization techniques\n",
    "verbose_prompt = \"\"\"\n",
    "I would like you to please help me with analyzing this data. \n",
    "Could you please take a look at the following information and \n",
    "provide me with some insights? I would really appreciate it if \n",
    "you could give me a detailed analysis.\n",
    "\"\"\"\n",
    "\n",
    "optimized_prompt = \"\"\"\n",
    "Analyze this data and provide insights:\n",
    "\"\"\"\n",
    "\n",
    "verbose_tokens = len(gpt4_tokenizer.encode(verbose_prompt))\n",
    "optimized_tokens = len(gpt4_tokenizer.encode(optimized_prompt))\n",
    "\n",
    "print(\"Prompt Optimization Example:\")\n",
    "print(f\"Verbose prompt: {verbose_tokens} tokens\")\n",
    "print(f\"Optimized prompt: {optimized_tokens} tokens\")\n",
    "print(f\"Token reduction: {verbose_tokens - optimized_tokens} tokens ({((verbose_tokens - optimized_tokens) / verbose_tokens * 100):.1f}% reduction)\")\n",
    "\n",
    "print(\"\\nBest Practices:\")\n",
    "print(\"1. Use concise language\")\n",
    "print(\"2. Avoid redundant phrases\")\n",
    "print(\"3. Use bullet points instead of long sentences\")\n",
    "print(\"4. Remove unnecessary politeness markers\")\n",
    "print(\"5. Use abbreviations where appropriate\")\n",
    "print(\"6. Structure data efficiently (JSON vs prose)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Interactive Token Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_token_counter(text):\n",
    "    \"\"\"Interactive function to analyze any text\"\"\"\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Character count: {len(text)}\")\n",
    "    print()\n",
    "    \n",
    "    # Different tokenization methods\n",
    "    word_tokens = word_tokenize(text)\n",
    "    gpt4_tokens = gpt4_tokenizer.encode(text)\n",
    "    bert_tokens = bert_tokenizer.tokenize(text)\n",
    "    \n",
    "    print(f\"Word tokens: {len(word_tokens)}\")\n",
    "    print(f\"GPT-4 tokens: {len(gpt4_tokens)}\")\n",
    "    print(f\"BERT tokens: {len(bert_tokens)}\")\n",
    "    print()\n",
    "    \n",
    "    # Show actual tokens\n",
    "    gpt4_decoded = [gpt4_tokenizer.decode([token]) for token in gpt4_tokens]\n",
    "    print(f\"GPT-4 tokenization: {gpt4_decoded}\")\n",
    "    print(f\"BERT tokenization: {bert_tokens}\")\n",
    "    \n",
    "    return {\n",
    "        'word_count': len(word_tokens),\n",
    "        'gpt4_count': len(gpt4_tokens),\n",
    "        'bert_count': len(bert_tokens)\n",
    "    }\n",
    "\n",
    "# Test with different examples\n",
    "test_cases = [\n",
    "    \"AI is amazing!\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Antidisestablishmentarianism\",\n",
    "    \"🚀 Rocket to the moon! 🌙\"\n",
    "]\n",
    "\n",
    "for test_text in test_cases:\n",
    "    print(\"=\" * 60)\n",
    "    interactive_token_counter(test_text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Tokenization Methods**:\n",
    "   - Word-level: Simple but large vocabulary\n",
    "   - Character-level: Small vocabulary but long sequences\n",
    "   - Subword (BPE): Balanced approach used by modern LLMs\n",
    "\n",
    "2. **Modern Tokenizers**:\n",
    "   - GPT models use tiktoken with BPE\n",
    "   - BERT uses WordPiece tokenization\n",
    "   - Each has different vocabulary and token counts\n",
    "\n",
    "3. **Practical Implications**:\n",
    "   - Token count affects API costs\n",
    "   - Different models tokenize differently\n",
    "   - Optimization can reduce costs significantly\n",
    "\n",
    "4. **Best Practices**:\n",
    "   - Monitor token usage for cost control\n",
    "   - Optimize prompts for efficiency\n",
    "   - Understand model-specific tokenization\n",
    "   - Consider special tokens and edge cases\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with your own text\n",
    "- Try different models and compare tokenization\n",
    "- Implement token counting in your applications\n",
    "- Optimize prompts for better efficiency"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
