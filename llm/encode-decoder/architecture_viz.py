def print_architecture():
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    TRANSFORMER ARCHITECTURE                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

INPUT: "Hello world"                    OUTPUT: "Hola mundo"
       â†“                                        â†‘
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Tokens  â”‚                              â”‚ Tokens  â”‚
   â”‚ [1,3,4] â”‚                              â”‚ [1,3,4] â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“                                        â†‘
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚Embeddingâ”‚                              â”‚Linear   â”‚
   â”‚+ Pos    â”‚                              â”‚Layer    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“                                        â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ENCODER     â”‚                        â”‚   DECODER     â”‚
â”‚               â”‚                        â”‚               â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚Self-Attn  â”‚ â”‚                        â”‚ â”‚Self-Attn  â”‚ â”‚
â”‚ â”‚(all words â”‚ â”‚                        â”‚ â”‚(causal)   â”‚ â”‚
â”‚ â”‚see each   â”‚ â”‚                        â”‚ â”‚           â”‚ â”‚
â”‚ â”‚other)     â”‚ â”‚                        â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                        â”‚       â†“       â”‚
â”‚       â†“       â”‚                        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                        â”‚ â”‚Cross-Attn â”‚ â”‚
â”‚ â”‚Feed       â”‚ â”‚                        â”‚ â”‚(looks at  â”‚ â”‚
â”‚ â”‚Forward    â”‚ â”‚                        â”‚ â”‚encoder)   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                        â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚               â”‚                        â”‚       â†“       â”‚
â”‚ (Repeat N     â”‚                        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  layers)      â”‚                        â”‚ â”‚Feed       â”‚ â”‚
â”‚               â”‚                        â”‚ â”‚Forward    â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
       â†“                                 â”‚               â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚ (Repeat N     â”‚
   â”‚Context  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚  layers)      â”‚
   â”‚Vector   â”‚                          â”‚               â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

KEY CONCEPTS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ” SELF-ATTENTION (Encoder):
   â€¢ Each word can see ALL other words in input
   â€¢ "Hello" can attend to "world" and vice versa
   â€¢ Creates rich contextual understanding

ğŸ” SELF-ATTENTION (Decoder):  
   â€¢ Each word can only see PREVIOUS words
   â€¢ When generating "mundo", can only see "hola"
   â€¢ Prevents cheating by looking ahead

ğŸ” CROSS-ATTENTION (Decoder):
   â€¢ Spanish words attend to English context
   â€¢ "hola" looks at "Hello world" to decide what to generate
   â€¢ This is where translation magic happens!

ğŸ¯ GENERATION PROCESS:
   1. Encoder processes "Hello world" â†’ context
   2. Decoder starts with <start> token
   3. Uses context + <start> â†’ generates "hola"  
   4. Uses context + <start> + "hola" â†’ generates "mundo"
   5. Uses context + <start> + "hola" + "mundo" â†’ generates <end>
""")

if __name__ == "__main__":
    print_architecture()
