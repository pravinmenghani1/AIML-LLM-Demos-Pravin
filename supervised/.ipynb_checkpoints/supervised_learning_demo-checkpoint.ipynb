{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning Demo: Step-by-Step Guide\n",
    "\n",
    "This notebook demonstrates the key concepts of supervised learning through an interactive classification example.\n",
    "\n",
    "## What is Supervised Learning?\n",
    "\n",
    "Supervised learning is a machine learning approach where we:\n",
    "1. **Train** a model using labeled data (input-output pairs)\n",
    "2. **Learn** patterns from this training data\n",
    "3. **Predict** outputs for new, unseen inputs\n",
    "\n",
    "Today we'll classify flowers based on their measurements!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Understanding Our Data\n",
    "\n",
    "We'll use the famous Iris dataset - it contains measurements of 150 flowers from 3 different species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features (measurements)\n",
    "y = iris.target  # Labels (species)\n",
    "\n",
    "# Create a DataFrame for easier visualization\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df['species'] = [iris.target_names[i] for i in y]\n",
    "\n",
    "print(\"üìä Dataset Overview:\")\n",
    "print(f\"Number of samples: {len(df)}\")\n",
    "print(f\"Number of features: {len(iris.feature_names)}\")\n",
    "print(f\"Species: {list(iris.target_names)}\")\n",
    "print(\"\\nüîç First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('üå∏ Iris Dataset: Feature Distributions by Species', fontsize=16, fontweight='bold')\n",
    "\n",
    "features = iris.feature_names\n",
    "for i, feature in enumerate(features):\n",
    "    row, col = i // 2, i % 2\n",
    "    for species in iris.target_names:\n",
    "        data = df[df['species'] == species][feature]\n",
    "        axes[row, col].hist(data, alpha=0.7, label=species, bins=15)\n",
    "    \n",
    "    axes[row, col].set_title(feature.replace('_', ' ').title())\n",
    "    axes[row, col].set_xlabel('Measurement (cm)')\n",
    "    axes[row, col].set_ylabel('Frequency')\n",
    "    axes[row, col].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Key Observation: Different species have different measurement patterns!\")\n",
    "print(\"This is what makes supervised learning possible - patterns in the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Supervised Learning Process\n",
    "\n",
    "### üéØ The Goal\n",
    "Given flower measurements ‚Üí Predict the species\n",
    "\n",
    "### üìö Training vs Testing\n",
    "We split our data into two parts:\n",
    "- **Training set**: Used to teach the model\n",
    "- **Test set**: Used to evaluate how well the model learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"üìä Data Split:\")\n",
    "print(f\"Training samples: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Testing samples: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Visualize the split\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Training set\n",
    "train_counts = np.bincount(y_train)\n",
    "ax1.bar(iris.target_names, train_counts, color=['skyblue', 'lightgreen', 'salmon'])\n",
    "ax1.set_title('üéì Training Set Distribution')\n",
    "ax1.set_ylabel('Number of Samples')\n",
    "\n",
    "# Test set\n",
    "test_counts = np.bincount(y_test)\n",
    "ax2.bar(iris.target_names, test_counts, color=['skyblue', 'lightgreen', 'salmon'])\n",
    "ax2.set_title('üß™ Test Set Distribution')\n",
    "ax2.set_ylabel('Number of Samples')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Data is evenly distributed across both sets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training Our First Model\n",
    "\n",
    "Let's start with a simple but powerful algorithm: **Logistic Regression**\n",
    "\n",
    "### How it works:\n",
    "1. Finds mathematical relationships between features and labels\n",
    "2. Creates decision boundaries to separate different classes\n",
    "3. Uses probability to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "print(\"ü§ñ Training Logistic Regression Model...\")\n",
    "\n",
    "# Scale the features for better performance\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the model\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"‚úÖ Model training completed!\")\n",
    "print(\"\\nüß† What the model learned:\")\n",
    "print(\"The model found mathematical patterns that connect flower measurements to species.\")\n",
    "print(\"It can now use these patterns to classify new flowers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Making Predictions\n",
    "\n",
    "Now let's see how our trained model performs on new, unseen data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = lr_model.predict(X_test_scaled)\n",
    "y_pred_proba = lr_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"üéØ Model Accuracy: {accuracy:.2%}\")\n",
    "print(f\"The model correctly classified {accuracy:.1%} of the test flowers!\")\n",
    "\n",
    "# Show some example predictions\n",
    "print(\"\\nüîç Example Predictions:\")\n",
    "for i in range(5):\n",
    "    actual = iris.target_names[y_test[i]]\n",
    "    predicted = iris.target_names[y_pred[i]]\n",
    "    confidence = np.max(y_pred_proba[i]) * 100\n",
    "    \n",
    "    status = \"‚úÖ\" if actual == predicted else \"‚ùå\"\n",
    "    print(f\"{status} Actual: {actual:12} | Predicted: {predicted:12} | Confidence: {confidence:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=iris.target_names, \n",
    "            yticklabels=iris.target_names, ax=ax1)\n",
    "ax1.set_title('üéØ Confusion Matrix')\n",
    "ax1.set_xlabel('Predicted')\n",
    "ax1.set_ylabel('Actual')\n",
    "\n",
    "# Prediction Confidence Distribution\n",
    "max_probas = np.max(y_pred_proba, axis=1)\n",
    "ax2.hist(max_probas, bins=20, alpha=0.7, color='green', edgecolor='black')\n",
    "ax2.set_title('üìä Prediction Confidence Distribution')\n",
    "ax2.set_xlabel('Confidence Level')\n",
    "ax2.set_ylabel('Number of Predictions')\n",
    "ax2.axvline(np.mean(max_probas), color='red', linestyle='--', \n",
    "           label=f'Average: {np.mean(max_probas):.2f}')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Insights:\")\n",
    "print(\"‚Ä¢ Diagonal values in confusion matrix = correct predictions\")\n",
    "print(\"‚Ä¢ Higher confidence values = more certain predictions\")\n",
    "print(f\"‚Ä¢ Average confidence: {np.mean(max_probas):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Comparing Different Algorithms\n",
    "\n",
    "Let's try a different algorithm: **Random Forest**\n",
    "- Uses multiple decision trees\n",
    "- Often more robust and accurate\n",
    "- Can handle complex patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest model\n",
    "print(\"üå≥ Training Random Forest Model...\")\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)  # Random Forest doesn't need scaling\n",
    "\n",
    "# Make predictions\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "print(f\"‚úÖ Random Forest Accuracy: {rf_accuracy:.2%}\")\n",
    "\n",
    "# Compare models\n",
    "print(\"\\nüèÜ Model Comparison:\")\n",
    "print(f\"Logistic Regression: {accuracy:.2%}\")\n",
    "print(f\"Random Forest:       {rf_accuracy:.2%}\")\n",
    "\n",
    "if rf_accuracy > accuracy:\n",
    "    print(\"üéâ Random Forest performs better!\")\n",
    "elif accuracy > rf_accuracy:\n",
    "    print(\"üéâ Logistic Regression performs better!\")\n",
    "else:\n",
    "    print(\"ü§ù Both models perform equally well!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest\n",
    "feature_importance = rf_model.feature_importances_\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(iris.feature_names, feature_importance, \n",
    "               color=['skyblue', 'lightgreen', 'salmon', 'gold'])\n",
    "plt.title('üåü Feature Importance (Random Forest)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, importance in zip(bars, feature_importance):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{importance:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Feature Importance Insights:\")\n",
    "most_important = iris.feature_names[np.argmax(feature_importance)]\n",
    "print(f\"‚Ä¢ Most important feature: {most_important}\")\n",
    "print(\"‚Ä¢ This tells us which measurements are most useful for classification\")\n",
    "print(\"‚Ä¢ Higher importance = more discriminative power\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Interactive Prediction\n",
    "\n",
    "Let's create a function to predict species for any flower measurements!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_flower_species(sepal_length, sepal_width, petal_length, petal_width):\n",
    "    \"\"\"\n",
    "    Predict flower species based on measurements\n",
    "    \"\"\"\n",
    "    # Prepare the input\n",
    "    measurements = np.array([[sepal_length, sepal_width, petal_length, petal_width]])\n",
    "    \n",
    "    # Scale for logistic regression\n",
    "    measurements_scaled = scaler.transform(measurements)\n",
    "    \n",
    "    # Get predictions from both models\n",
    "    lr_pred = lr_model.predict(measurements_scaled)[0]\n",
    "    lr_proba = lr_model.predict_proba(measurements_scaled)[0]\n",
    "    \n",
    "    rf_pred = rf_model.predict(measurements)[0]\n",
    "    rf_proba = rf_model.predict_proba(measurements)[0]\n",
    "    \n",
    "    print(f\"üå∏ Flower Measurements:\")\n",
    "    print(f\"   Sepal Length: {sepal_length} cm\")\n",
    "    print(f\"   Sepal Width:  {sepal_width} cm\")\n",
    "    print(f\"   Petal Length: {petal_length} cm\")\n",
    "    print(f\"   Petal Width:  {petal_width} cm\")\n",
    "    print(\"\\nü§ñ Model Predictions:\")\n",
    "    print(f\"   Logistic Regression: {iris.target_names[lr_pred]} ({np.max(lr_proba):.1%} confidence)\")\n",
    "    print(f\"   Random Forest:       {iris.target_names[rf_pred]} ({np.max(rf_proba):.1%} confidence)\")\n",
    "    \n",
    "    # Show probability breakdown\n",
    "    print(\"\\nüìä Probability Breakdown (Random Forest):\")\n",
    "    for i, species in enumerate(iris.target_names):\n",
    "        print(f\"   {species:12}: {rf_proba[i]:.1%}\")\n",
    "\n",
    "# Example predictions\n",
    "print(\"üß™ Example Predictions:\\n\")\n",
    "\n",
    "print(\"Example 1: Small flower\")\n",
    "predict_flower_species(4.5, 2.5, 1.2, 0.3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Example 2: Medium flower\")\n",
    "predict_flower_species(6.0, 3.0, 4.0, 1.2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Example 3: Large flower\")\n",
    "predict_flower_species(7.5, 3.2, 6.0, 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Key Takeaways\n",
    "\n",
    "### üéì What We Learned About Supervised Learning:\n",
    "\n",
    "1. **Data is Key**: Quality labeled data is essential\n",
    "2. **Train-Test Split**: Always evaluate on unseen data\n",
    "3. **Multiple Algorithms**: Different algorithms have different strengths\n",
    "4. **Feature Importance**: Some measurements matter more than others\n",
    "5. **Confidence Matters**: Models provide probability estimates\n",
    "\n",
    "### üîÑ The Supervised Learning Workflow:\n",
    "```\n",
    "Data Collection ‚Üí Data Preparation ‚Üí Model Training ‚Üí Evaluation ‚Üí Prediction\n",
    "```\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "- Try with your own dataset\n",
    "- Experiment with different algorithms\n",
    "- Learn about feature engineering\n",
    "- Explore cross-validation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization: Decision boundary (2D projection)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "X_train_2d = pca.transform(X_train)\n",
    "X_test_2d = pca.transform(X_test)\n",
    "\n",
    "# Train a simple model on 2D data\n",
    "simple_model = LogisticRegression()\n",
    "simple_model.fit(X_train_2d, y_train)\n",
    "\n",
    "# Create decision boundary\n",
    "h = 0.02\n",
    "x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1\n",
    "y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "Z = simple_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "\n",
    "# Plot data points\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, species in enumerate(iris.target_names):\n",
    "    mask = y == i\n",
    "    plt.scatter(X_2d[mask, 0], X_2d[mask, 1], \n",
    "               c=colors[i], label=species, s=50, alpha=0.7)\n",
    "\n",
    "plt.title('üéØ Decision Boundaries in 2D Space', fontsize=16, fontweight='bold')\n",
    "plt.xlabel(f'First Principal Component ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "plt.ylabel(f'Second Principal Component ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"üé® This visualization shows how the model creates decision boundaries\")\n",
    "print(\"to separate different classes in the feature space!\")\n",
    "print(f\"\\nüìà Total variance explained by 2D projection: {sum(pca.explained_variance_ratio_):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully completed a comprehensive supervised learning demo! \n",
    "\n",
    "You now understand:\n",
    "- How supervised learning works\n",
    "- The importance of train-test splits\n",
    "- How to evaluate model performance\n",
    "- How different algorithms compare\n",
    "- How to make predictions on new data\n",
    "\n",
    "**Try modifying the code above to:**\n",
    "- Use different train-test split ratios\n",
    "- Try other algorithms (SVM, Decision Trees, etc.)\n",
    "- Experiment with feature selection\n",
    "- Test with your own measurements!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
