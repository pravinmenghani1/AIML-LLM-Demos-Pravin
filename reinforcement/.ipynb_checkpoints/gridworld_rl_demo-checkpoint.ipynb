{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Reinforcement Learning Demo: Grid World Navigation\n",
    "\n",
    "## Problem Statement\n",
    "**Goal**: Train an AI agent to navigate from START to GOAL while avoiding obstacles\n",
    "\n",
    "**Environment**: 5x5 grid world\n",
    "- üü¢ **Start**: Position (0,0) \n",
    "- üéØ **Goal**: Position (4,4)\n",
    "- üö´ **Obstacles**: Fixed barriers at (1,1), (2,2), (3,1)\n",
    "\n",
    "**Actions**: 4 directions (‚Üë ‚Üí ‚Üì ‚Üê)\n",
    "\n",
    "**Rewards**:\n",
    "- +100 points for reaching the goal\n",
    "- -1 point for each step (encourages efficiency)\n",
    "- -10 points for hitting obstacles\n",
    "\n",
    "**Learning Algorithm**: Q-Learning with Œµ-greedy exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.start = (0, 0)\n",
    "        self.goal = (size-1, size-1)\n",
    "        self.obstacles = [(1, 1), (2, 2), (3, 1)]\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.pos = self.start\n",
    "        return self.pos\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Actions: 0=up, 1=right, 2=down, 3=left\n",
    "        moves = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n",
    "        new_pos = (self.pos[0] + moves[action][0], self.pos[1] + moves[action][1])\n",
    "        \n",
    "        # Check boundaries\n",
    "        if (new_pos[0] < 0 or new_pos[0] >= self.size or \n",
    "            new_pos[1] < 0 or new_pos[1] >= self.size):\n",
    "            new_pos = self.pos\n",
    "        \n",
    "        # Check obstacles\n",
    "        if new_pos in self.obstacles:\n",
    "            new_pos = self.pos\n",
    "        \n",
    "        self.pos = new_pos\n",
    "        \n",
    "        # Rewards\n",
    "        if self.pos == self.goal:\n",
    "            reward = 100\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1\n",
    "            done = False\n",
    "        \n",
    "        return self.pos, reward, done\n",
    "    \n",
    "    def visualize(self, agent_pos=None, path=None):\n",
    "        grid = np.full((self.size, self.size), '‚¨ú', dtype=object)\n",
    "        \n",
    "        # Mark path if provided\n",
    "        if path:\n",
    "            for pos in path[1:-1]:  # Exclude start and goal\n",
    "                if pos not in self.obstacles:\n",
    "                    grid[pos] = 'üîµ'\n",
    "        \n",
    "        # Mark special positions\n",
    "        grid[self.start] = 'üü¢'  # Start\n",
    "        grid[self.goal] = 'üéØ'   # Goal\n",
    "        for obs in self.obstacles:\n",
    "            grid[obs] = 'üö´'     # Obstacles\n",
    "        \n",
    "        # Mark current agent position\n",
    "        if agent_pos and agent_pos != self.start and agent_pos != self.goal:\n",
    "            grid[agent_pos] = 'ü§ñ'\n",
    "        \n",
    "        print(\"Grid World:\")\n",
    "        for row in grid:\n",
    "            print(' '.join(row))\n",
    "        print(\"üü¢=Start, üéØ=Goal, üö´=Obstacle, ü§ñ=Agent, üîµ=Path\")\n",
    "\n",
    "# Create and visualize environment\n",
    "env = GridWorld()\n",
    "env.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, n_states, n_actions, lr=0.1, gamma=0.9, epsilon=1.0, epsilon_decay=0.995):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.q_table = np.zeros((n_states, n_actions))\n",
    "        \n",
    "        # For tracking learning\n",
    "        self.episode_rewards = []\n",
    "        self.episode_steps = []\n",
    "        self.success_rate = []\n",
    "    \n",
    "    def state_to_index(self, state, grid_size):\n",
    "        return state[0] * grid_size + state[1]\n",
    "    \n",
    "    def choose_action(self, state, grid_size):\n",
    "        state_idx = self.state_to_index(state, grid_size)\n",
    "        \n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.n_actions - 1)  # Explore\n",
    "        return np.argmax(self.q_table[state_idx])  # Exploit\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done, grid_size):\n",
    "        state_idx = self.state_to_index(state, grid_size)\n",
    "        next_state_idx = self.state_to_index(next_state, grid_size)\n",
    "        \n",
    "        # Q-Learning update rule\n",
    "        current_q = self.q_table[state_idx, action]\n",
    "        next_max_q = 0 if done else np.max(self.q_table[next_state_idx])\n",
    "        target = reward + self.gamma * next_max_q\n",
    "        \n",
    "        self.q_table[state_idx, action] = current_q + self.lr * (target - current_q)\n",
    "        \n",
    "        # Decay exploration\n",
    "        if self.epsilon > 0.01:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def get_policy_visualization(self, env):\n",
    "        \"\"\"Show learned policy as arrows\"\"\"\n",
    "        actions = ['‚Üë', '‚Üí', '‚Üì', '‚Üê']\n",
    "        grid = np.full((env.size, env.size), ' ', dtype=object)\n",
    "        \n",
    "        for i in range(env.size):\n",
    "            for j in range(env.size):\n",
    "                if (i, j) == env.start:\n",
    "                    grid[i, j] = 'üü¢'\n",
    "                elif (i, j) == env.goal:\n",
    "                    grid[i, j] = 'üéØ'\n",
    "                elif (i, j) in env.obstacles:\n",
    "                    grid[i, j] = 'üö´'\n",
    "                else:\n",
    "                    state_idx = self.state_to_index((i, j), env.size)\n",
    "                    best_action = np.argmax(self.q_table[state_idx])\n",
    "                    grid[i, j] = actions[best_action]\n",
    "        \n",
    "        return grid\n",
    "\n",
    "# Create agent\n",
    "agent = QLearningAgent(n_states=25, n_actions=4)\n",
    "print(f\"‚úÖ Q-Learning Agent created!\")\n",
    "print(f\"   Q-table shape: {agent.q_table.shape}\")\n",
    "print(f\"   Learning rate: {agent.lr}\")\n",
    "print(f\"   Discount factor: {agent.gamma}\")\n",
    "print(f\"   Initial exploration rate: {agent.epsilon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Random Agent Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_random_agent(env, episodes=10):\n",
    "    print(\"üé≤ Testing Random Agent (No Learning):\")\n",
    "    scores = []\n",
    "    successes = 0\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        for step in range(50):  # Max 50 steps\n",
    "            action = random.randint(0, 3)  # Random action\n",
    "            state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                successes += 1\n",
    "                break\n",
    "        \n",
    "        scores.append(total_reward)\n",
    "        status = \"üéØ SUCCESS\" if done and state == env.goal else \"‚ùå FAILED\"\n",
    "        print(f\"  Episode {episode + 1}: {total_reward:3.0f} points, {steps:2d} steps - {status}\")\n",
    "    \n",
    "    avg_score = np.mean(scores)\n",
    "    success_rate = (successes / episodes) * 100\n",
    "    \n",
    "    print(f\"\\nüìä Random Agent Results:\")\n",
    "    print(f\"   Average Score: {avg_score:.1f}\")\n",
    "    print(f\"   Success Rate: {success_rate:.0f}%\")\n",
    "    \n",
    "    return scores, success_rate\n",
    "\n",
    "random_scores, random_success = test_random_agent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training with Live Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_visualization(env, agent, episodes=200, show_every=50):\n",
    "    print(\"üß† Training Q-Learning Agent with Live Updates:\")\n",
    "    \n",
    "    all_rewards = []\n",
    "    recent_successes = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        path = [state]\n",
    "        \n",
    "        # Run episode\n",
    "        for step in range(50):\n",
    "            action = agent.choose_action(state, env.size)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            agent.learn(state, action, reward, next_state, done, env.size)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            path.append(state)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        all_rewards.append(total_reward)\n",
    "        recent_successes.append(1 if done and state == env.goal else 0)\n",
    "        \n",
    "        # Show progress every N episodes\n",
    "        if (episode + 1) % show_every == 0:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Calculate recent performance\n",
    "            recent_avg = np.mean(all_rewards[-show_every:])\n",
    "            recent_success_rate = np.mean(recent_successes[-show_every:]) * 100\n",
    "            \n",
    "            print(f\"üß† Training Progress - Episode {episode + 1}/{episodes}\")\n",
    "            print(f\"   Recent Average Score: {recent_avg:.1f}\")\n",
    "            print(f\"   Recent Success Rate: {recent_success_rate:.0f}%\")\n",
    "            print(f\"   Exploration Rate (Œµ): {agent.epsilon:.3f}\")\n",
    "            print(f\"   Last Episode: {total_reward:.0f} points, {steps} steps\")\n",
    "            \n",
    "            # Show current policy\n",
    "            print(\"\\nüó∫Ô∏è Current Learned Policy:\")\n",
    "            policy_grid = agent.get_policy_visualization(env)\n",
    "            for row in policy_grid:\n",
    "                print(' '.join(row))\n",
    "            \n",
    "            # Show last episode path\n",
    "            if len(path) <= 15:  # Only show if path is reasonable\n",
    "                print(f\"\\nüõ§Ô∏è Last Episode Path: {' ‚Üí '.join([str(p) for p in path])}\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            time.sleep(1)  # Brief pause to see updates\n",
    "    \n",
    "    return all_rewards\n",
    "\n",
    "# Train the agent\n",
    "training_rewards = train_with_visualization(env, agent, episodes=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_trained_agent(env, agent, episodes=10):\n",
    "    print(\"üéØ Testing Trained Agent (Pure Exploitation):\")\n",
    "    \n",
    "    # Temporarily disable exploration\n",
    "    original_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0\n",
    "    \n",
    "    scores = []\n",
    "    successes = 0\n",
    "    all_paths = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        path = [state]\n",
    "        \n",
    "        for step in range(50):\n",
    "            action = agent.choose_action(state, env.size)\n",
    "            state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            path.append(state)\n",
    "            \n",
    "            if done:\n",
    "                successes += 1\n",
    "                break\n",
    "        \n",
    "        scores.append(total_reward)\n",
    "        all_paths.append(path)\n",
    "        status = \"üéØ SUCCESS\" if done and state == env.goal else \"‚ùå FAILED\"\n",
    "        print(f\"  Test {episode + 1}: {total_reward:3.0f} points, {steps:2d} steps - {status}\")\n",
    "        \n",
    "        # Show path for first few tests\n",
    "        if episode < 3 and len(path) <= 15:\n",
    "            print(f\"    Path: {' ‚Üí '.join([str(p) for p in path])}\")\n",
    "    \n",
    "    # Restore original epsilon\n",
    "    agent.epsilon = original_epsilon\n",
    "    \n",
    "    avg_score = np.mean(scores)\n",
    "    success_rate = (successes / episodes) * 100\n",
    "    \n",
    "    print(f\"\\nüìä Trained Agent Results:\")\n",
    "    print(f\"   Average Score: {avg_score:.1f}\")\n",
    "    print(f\"   Success Rate: {success_rate:.0f}%\")\n",
    "    \n",
    "    return scores, success_rate, all_paths\n",
    "\n",
    "trained_scores, trained_success, paths = test_trained_agent(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Final Policy Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üó∫Ô∏è Final Learned Policy:\")\n",
    "policy_grid = agent.get_policy_visualization(env)\n",
    "for i, row in enumerate(policy_grid):\n",
    "    print(f\"{i}  \" + \" \".join(row))\n",
    "\n",
    "print(\"\\nLegend: üü¢=Start, üéØ=Goal, üö´=Obstacle, ‚Üë‚Üí‚Üì‚Üê=Best Action\")\n",
    "\n",
    "# Show optimal path\n",
    "if paths and len(paths[0]) <= 15:\n",
    "    print(f\"\\nüõ§Ô∏è Optimal Path Found: {' ‚Üí '.join([str(p) for p in paths[0]])}\")\n",
    "    print(f\"   Path Length: {len(paths[0]) - 1} steps\")\n",
    "    \n",
    "    # Visualize the optimal path\n",
    "    print(\"\\nüó∫Ô∏è Path Visualization:\")\n",
    "    env.visualize(path=paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Learning Analysis & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "print(\"üìä LEARNING RESULTS COMPARISON:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "random_avg = np.mean(random_scores)\n",
    "trained_avg = np.mean(trained_scores)\n",
    "improvement = trained_avg - random_avg\n",
    "improvement_pct = (improvement / abs(random_avg)) * 100 if random_avg != 0 else float('inf')\n",
    "\n",
    "print(f\"üé≤ Random Agent:\")\n",
    "print(f\"   Average Score: {random_avg:.1f} points\")\n",
    "print(f\"   Success Rate: {random_success:.0f}%\")\n",
    "\n",
    "print(f\"\\nüß† Trained Agent:\")\n",
    "print(f\"   Average Score: {trained_avg:.1f} points\")\n",
    "print(f\"   Success Rate: {trained_success:.0f}%\")\n",
    "\n",
    "print(f\"\\nüöÄ Improvement:\")\n",
    "print(f\"   Score Improvement: {improvement:.1f} points ({improvement_pct:.1f}%)\")\n",
    "print(f\"   Success Rate Improvement: {trained_success - random_success:.0f} percentage points\")\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Learning curve\n",
    "plt.subplot(1, 3, 1)\n",
    "window = 20\n",
    "smoothed_rewards = []\n",
    "for i in range(len(training_rewards)):\n",
    "    start = max(0, i - window + 1)\n",
    "    smoothed_rewards.append(np.mean(training_rewards[start:i+1]))\n",
    "\n",
    "plt.plot(training_rewards, alpha=0.3, color='lightblue', label='Raw scores')\n",
    "plt.plot(smoothed_rewards, color='blue', linewidth=2, label=f'Moving average ({window})')\n",
    "plt.title('üß† Learning Progress')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Performance comparison\n",
    "plt.subplot(1, 3, 2)\n",
    "agents = ['Random\\nAgent', 'Trained\\nAgent']\n",
    "scores = [random_avg, trained_avg]\n",
    "colors = ['red', 'green']\n",
    "bars = plt.bar(agents, scores, color=colors, alpha=0.7)\n",
    "plt.title('üèÜ Performance Comparison')\n",
    "plt.ylabel('Average Score')\n",
    "plt.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, scores):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "             f'{score:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Success rate comparison\n",
    "plt.subplot(1, 3, 3)\n",
    "success_rates = [random_success, trained_success]\n",
    "bars = plt.bar(agents, success_rates, color=colors, alpha=0.7)\n",
    "plt.title('üéØ Success Rate Comparison')\n",
    "plt.ylabel('Success Rate (%)')\n",
    "plt.ylim(0, 105)\n",
    "plt.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, rate in zip(bars, success_rates):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "             f'{rate:.0f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéâ Training Complete! The agent learned to navigate optimally!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Reinforcement Learning Concepts Demonstrated\n",
    "\n",
    "### 1. **Trial and Error Learning**\n",
    "- Agent started with random actions (low success rate)\n",
    "- Gradually improved through experience\n",
    "- Achieved optimal performance after training\n",
    "\n",
    "### 2. **Q-Learning Algorithm**\n",
    "- **Q-Table**: Stores expected rewards for each state-action pair\n",
    "- **Update Rule**: Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥¬∑max(Q(s',a')) - Q(s,a)]\n",
    "- **Convergence**: Q-values converged to optimal policy\n",
    "\n",
    "### 3. **Exploration vs Exploitation**\n",
    "- **Œµ-greedy strategy**: Balance between trying new actions and using known good ones\n",
    "- **Epsilon decay**: Started exploring (Œµ=1.0), ended exploiting (Œµ‚âà0.01)\n",
    "- **Learning phases**: Exploration ‚Üí Mixed ‚Üí Pure exploitation\n",
    "\n",
    "### 4. **Reward Shaping**\n",
    "- **Goal reward**: +100 for reaching target\n",
    "- **Step penalty**: -1 per step (encourages efficiency)\n",
    "- **Obstacle penalty**: -10 for hitting barriers\n",
    "\n",
    "### 5. **Policy Learning**\n",
    "- **Policy**: Mapping from states to actions\n",
    "- **Optimal policy**: Found shortest path avoiding obstacles\n",
    "- **Deterministic**: Consistent actions in each state\n",
    "\n",
    "### üöÄ **Real-World Applications**\n",
    "- **Robotics**: Path planning and navigation\n",
    "- **Game AI**: Strategic decision making\n",
    "- **Resource Management**: Optimal allocation\n",
    "- **Trading**: Portfolio optimization\n",
    "- **Autonomous Vehicles**: Route planning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
