{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Demo: Lunar Lander\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium[box2d] numpy matplotlib pandas tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "print(f\"Actions: {env.action_space.n}\")\n",
    "print(f\"Observations: {env.observation_space.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, n_actions=4, lr=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.995):\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.q_table = defaultdict(lambda: np.zeros(n_actions))\n",
    "        \n",
    "    def discretize_state(self, state):\n",
    "        return tuple(np.round(state * 10).astype(int))\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        discrete_state = self.discretize_state(state)\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.n_actions - 1)\n",
    "        return np.argmax(self.q_table[discrete_state])\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        discrete_state = self.discretize_state(state)\n",
    "        discrete_next_state = self.discretize_state(next_state)\n",
    "        \n",
    "        current_q = self.q_table[discrete_state][action]\n",
    "        next_max_q = 0 if done else np.max(self.q_table[discrete_next_state])\n",
    "        target = reward + self.gamma * next_max_q\n",
    "        \n",
    "        self.q_table[discrete_state][action] = current_q + self.lr * (target - current_q)\n",
    "        \n",
    "        if self.epsilon > 0.01:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QLearningAgent()\n",
    "scores = []\n",
    "\n",
    "for episode in tqdm(range(500)):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        agent.learn(state, action, reward, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    scores.append(total_reward)\n",
    "    \n",
    "    if (episode + 1) % 100 == 0:\n",
    "        avg_score = np.mean(scores[-100:])\n",
    "        print(f\"Episode {episode + 1}: Avg Score = {avg_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(scores)\n",
    "plt.title('Training Scores')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "moving_avg = pd.Series(scores).rolling(50).mean()\n",
    "plt.plot(moving_avg)\n",
    "plt.title('Moving Average (50 episodes)')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final average score: {np.mean(scores[-100:]):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
