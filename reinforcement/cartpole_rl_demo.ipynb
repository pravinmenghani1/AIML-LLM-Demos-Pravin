{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Reinforcement Learning Demo: CartPole Balance\n",
    "\n",
    "## Problem Statement\n",
    "**Goal**: Train an AI agent to balance a pole on a moving cart\n",
    "\n",
    "**Environment**: CartPole-v1 from OpenAI Gymnasium\n",
    "- **State**: [cart_position, cart_velocity, pole_angle, pole_velocity]\n",
    "- **Actions**: 2 discrete actions (0=Push Left, 1=Push Right)\n",
    "- **Reward**: +1 for each timestep the pole remains upright\n",
    "- **Episode End**: Pole angle > 12Â°, cart position > 2.4, or 500 steps\n",
    "- **Success**: Average score â‰¥ 475 over 100 consecutive episodes\n",
    "\n",
    "**Learning Algorithm**: Q-Learning with state discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "print(\"âœ… Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "print(\"ðŸŽ¯ CartPole Environment Created!\")\n",
    "print(f\"   State Space: {env.observation_space}\")\n",
    "print(f\"   Action Space: {env.action_space} (0=Left, 1=Right)\")\n",
    "print(f\"   Max Episode Steps: 500\")\n",
    "\n",
    "# Test environment\n",
    "state, _ = env.reset()\n",
    "print(f\"\\nðŸ“Š Sample State: {state}\")\n",
    "print(\"   [cart_pos, cart_vel, pole_angle, pole_vel]\")\n",
    "print(f\"   Cart Position: {state[0]:.3f}\")\n",
    "print(f\"   Cart Velocity: {state[1]:.3f}\")\n",
    "print(f\"   Pole Angle: {state[2]:.3f} radians ({np.degrees(state[2]):.1f}Â°)\")\n",
    "print(f\"   Pole Velocity: {state[3]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Q-Learning Agent with State Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, n_actions=2, lr=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.995):\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = 0.01\n",
    "        \n",
    "        # Q-table as dictionary for discrete states\n",
    "        self.q_table = defaultdict(lambda: np.zeros(n_actions))\n",
    "        \n",
    "        # Tracking metrics\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.epsilons = []\n",
    "        \n",
    "    def discretize_state(self, state):\n",
    "        \"\"\"Convert continuous state to discrete bins\"\"\"\n",
    "        # Define bins for each state dimension\n",
    "        bins = [10, 10, 10, 10]  # Number of bins per dimension\n",
    "        ranges = [\n",
    "            (-2.4, 2.4),    # Cart position\n",
    "            (-3.0, 3.0),    # Cart velocity  \n",
    "            (-0.2, 0.2),    # Pole angle\n",
    "            (-3.0, 3.0)     # Pole velocity\n",
    "        ]\n",
    "        \n",
    "        discrete_state = []\n",
    "        for i, (value, (low, high)) in enumerate(zip(state, ranges)):\n",
    "            # Clip value to range\n",
    "            value = max(low, min(high, value))\n",
    "            # Convert to discrete bin\n",
    "            bin_index = int((value - low) / (high - low) * (bins[i] - 1))\n",
    "            discrete_state.append(bin_index)\n",
    "        \n",
    "        return tuple(discrete_state)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "        discrete_state = self.discretize_state(state)\n",
    "        \n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.n_actions - 1)  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.q_table[discrete_state])  # Exploit\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Q-learning update\"\"\"\n",
    "        discrete_state = self.discretize_state(state)\n",
    "        discrete_next_state = self.discretize_state(next_state)\n",
    "        \n",
    "        # Current Q-value\n",
    "        current_q = self.q_table[discrete_state][action]\n",
    "        \n",
    "        # Next state's maximum Q-value\n",
    "        if done:\n",
    "            next_max_q = 0\n",
    "        else:\n",
    "            next_max_q = np.max(self.q_table[discrete_next_state])\n",
    "        \n",
    "        # Q-learning update rule\n",
    "        target = reward + self.gamma * next_max_q\n",
    "        self.q_table[discrete_state][action] = current_q + self.lr * (target - current_q)\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Create agent\n",
    "agent = QLearningAgent()\n",
    "print(\"ðŸ§  Q-Learning Agent Created!\")\n",
    "print(f\"   Learning Rate: {agent.lr}\")\n",
    "print(f\"   Discount Factor: {agent.gamma}\")\n",
    "print(f\"   Initial Exploration: {agent.epsilon}\")\n",
    "print(f\"   Epsilon Decay: {agent.epsilon_decay}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
